\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={MINSTDeepLearning.r},
            pdfauthor={soondra},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{MINSTDeepLearning.r}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{soondra}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-05-26}


\begin{document}
\maketitle

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Veamos un ejemplo concreto de una red neuronal que usa el paquete Keras R para aprender a clasificar los dígitos escritos a mano. El problema que intentamos resolver aquí es clasificar las imágenes en escala de grises de los dígitos escritos a mano (28 píxeles por 28 píxeles) en 10 categorías (números del 0 a 9). Usaremos el conjunto de datos MNIST, que consta de 60,000 imágenes de entrenamiento, más 10,000 imágenes de prueba, reunidas por el Instituto Nacional de Estándares y Tecnología (el NIST en MNIST) en la década de 1980. }

\CommentTok{#El conjunto de datos MNIST viene precargado en Keras, en forma de listas `train` y` test`, cada una de las cuales incluye un conjunto de imágenes (`x`) y etiquetas asociadas (` y`):}

\KeywordTok{library}\NormalTok{(keras)}

\NormalTok{mnist <-}\StringTok{ }\KeywordTok{dataset_mnist}\NormalTok{()}
\NormalTok{train_images <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{x}
\NormalTok{train_labels <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{y}
\NormalTok{test_images <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{x}
\NormalTok{test_labels <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{y}

\CommentTok{#Veamos la dimensión: es un tensor de 3 dimensiones}
\KeywordTok{length}\NormalTok{(}\KeywordTok{dim}\NormalTok{(train_images))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(train_images)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 60000    28    28
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#El tipo de datos es}

\KeywordTok{typeof}\NormalTok{(train_images)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Podemos acceder a una imagen del conjunto de entrenamiento}

\NormalTok{digit <-}\StringTok{ }\NormalTok{train_images[}\DecValTok{5}\NormalTok{,,]}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.raster}\NormalTok{(digit, }\DataTypeTok{max =} \DecValTok{255}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{MINSTDeepLearning_files/figure-latex/unnamed-chunk-1-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#`train_images` y `train_labels` forman el _conjunto de entrenamiento_ a partir de los cuales se realizará el aprendizaje.}
\CommentTok{#El modelo se probará con el _conjunto de test_, `test_images` y` test_labels`. Las imágenes se codifican en arrays 3D, como se puede observar utilizando la función `str()` y las etiquetas son un conjunto de dígitos 1D, que van de 0 a 9. Hay una correspondencia de uno a uno entre las imágenes y las etiquetas.}

\CommentTok{#La función R `str ()` es una forma conveniente de echar un vistazo rápido a la estructura de una matriz. Vamos a usarlo para echar un vistazo a los datos de entrenamiento:}
  
 \KeywordTok{str}\NormalTok{(train_images)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{str}\NormalTok{(train_labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#El flujo de trabajo será el siguiente: primero alimentaremos la red neuronal con los datos de entrenamiento, `train_images` y `train_labels`. La red aprenderá a asociar imágenes y etiquetas. Finalmente, le pediremos a la red que produzca predicciones para `test_images`, y verificaremos si estas predicciones coinciden con las etiquetas de `test_labels`.}

\NormalTok{network <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{512}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"relu"}\NormalTok{, }\DataTypeTok{input_shape =} \KeywordTok{c}\NormalTok{(}\DecValTok{28} \OperatorTok{*}\StringTok{ }\DecValTok{28}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{10}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"softmax"}\NormalTok{)}

\KeywordTok{summary}\NormalTok{(network)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense (Dense)                    (None, 512)                   401920      
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 10)                    5130        
## ===========================================================================
## Total params: 407,050
## Trainable params: 407,050
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#En las líneas anteriores se define la arquitectura de la red neuronal como una secuencia de dos capas denso-conectadas. La primera consta de 512 unidades de salida, asociadas a las entradas que de cada uno de los pixels (28*28) de la imagen. La segunda capa tiene 10 unidades, cada una de ellas asociada a un dígito (del 0 al 9). La función de activación _softmax_ hace que se retorne un array de 10 valores de probabilidad, cuya suma obviamente debe ser 1. }

\CommentTok{#Una vez que la arquitectura de la red está definida, es preciso definir el resto de ingredientes. En las siguientes líneas indicamos como va a ser el entrenamiento, en concreto como se optimizará la red, cual es la función de pérdida, y cual será la métrica de evaluación. En este caso, la función de pérdida, es decir, la función a minimizar, es __categorical_crossentropy__. El optimizador, es decir el mecanismo de ajuste de los pesos es `rmsprop`, es decir __descenso del gradiente mini-batch__ (seleccionando solo un conjunto de pesos). }

\NormalTok{network }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(}
  \DataTypeTok{optimizer =} \StringTok{"rmsprop"}\NormalTok{,}
  \DataTypeTok{loss =} \StringTok{"categorical_crossentropy"}\NormalTok{,}
  \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\CommentTok{# El operador %>% pasa el objeto que está a su izquierda como }
\CommentTok{# primer argumento de la función de su derecha}

\CommentTok{#Antes de entrenar, reconfiguramos los datos a la forma que la red espera y __escalamos para que todos los valores estén en el intervalo__ `[0, 1]`. Anteriormente, nuestras imágenes de entrenamiento, por ejemplo, se almacenaban en una matriz de forma `(60000, 28, 28)` de tipo entero con valores en el intervalo `[0, 255]`. Lo transformamos matriz de orden `(60000, 28 * 28)` con valores entre 0 y 1.}


\NormalTok{train_images <-}\StringTok{ }\KeywordTok{array_reshape}\NormalTok{(train_images, }\KeywordTok{c}\NormalTok{(}\DecValTok{60000}\NormalTok{, }\DecValTok{28} \OperatorTok{*}\StringTok{ }\DecValTok{28}\NormalTok{))}
\NormalTok{train_images <-}\StringTok{ }\NormalTok{train_images }\OperatorTok{/}\StringTok{ }\DecValTok{255}

\CommentTok{#El conjunto de entrenamiento está almacenado en un 2-tensor (una matriz) de dimensión (60000, 784) }
\NormalTok{test_images <-}\StringTok{ }\KeywordTok{array_reshape}\NormalTok{(test_images, }\KeywordTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{28} \OperatorTok{*}\StringTok{ }\DecValTok{28}\NormalTok{))}
\NormalTok{test_images <-}\StringTok{ }\NormalTok{test_images }\OperatorTok{/}\StringTok{ }\DecValTok{255}

\NormalTok{train_labels <-}\StringTok{ }\KeywordTok{to_categorical}\NormalTok{(train_labels)}
\NormalTok{test_labels <-}\StringTok{ }\KeywordTok{to_categorical}\NormalTok{(test_labels)}

\CommentTok{#Finalmente ya podemos entrenar la red, que en Keras se hace con la función `fit`. En este caso la red se entrena seleccionado subconjuntos de 128 ejemplos (_batches_) sobre los que se entrena 5 veces (__epoch__). En cada iteración, la red computará los gradientes de los pesos con respecto a la función de pérdida pérdida sobre cada _batch_, y actualizará los pesos}
\CommentTok{#en consecuencia. Después de estas 5 iteraciones, la red habrá realizado 2,345 gradientes.}
\CommentTok{#actualizaciones (469 por _epoch_), y la pérdida de la red será lo suficientemente baja como para que La red será capaz de clasificar los dígitos escritos a mano con alta precisión.}

\NormalTok{network }\OperatorTok{%>%}\StringTok{ }\KeywordTok{fit}\NormalTok{(train_images, train_labels, }\DataTypeTok{epochs =} \DecValTok{5}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{128}\NormalTok{)}

\CommentTok{#Se muestra el valor de la función de pérdida de la red y los datos de entrenamiento sobre los datos de entrenamiento.}
\CommentTok{#El porcentaje de acierto en entrenamiento es de 98.9% en los datos de entrenamiento. Comprobemos que nuestro modelo también funciona bien en el conjunto de prueba:}
  
\NormalTok{metrics <-}\StringTok{ }\NormalTok{network }\OperatorTok{%>%}\StringTok{ }\KeywordTok{evaluate}\NormalTok{(test_images, test_labels, }\DataTypeTok{verbose =} \DecValTok{0}\NormalTok{)}
\NormalTok{metrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $loss
## [1] 0.0633433
## 
## $acc
## [1] 0.9813
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Con este pequeño ejemplo puedes observar como contruir y entrenar una red neuronal para clasificar dígitos escritos a mano en menos de 20 líneas de código. }


\CommentTok{#Las redes neuronales convolucionales han tenido mucho éxito porque trabajan muy bien con imágenes. }
\CommentTok{#En general, las redes neuronales convolucionales van a estar construidas con una estructura que contendrá 3 tipos distintos de capas:}
  

\CommentTok{#* Una capa convolucional, que es la que le da le nombre a la red.}
\CommentTok{#* Una capa de reducción o de pooling, la cual va a reducir la cantidad de parámetros al quedarse con las características más comunes.}
\CommentTok{#* Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.}

\CommentTok{#Las redes neuronales convolucionales se distinguen de cualquier otra red neuronal en que utilizan un operación llamada convolución en alguna de sus capas en lugar de utilizar la multiplicación de matrices que se aplica generalmente. La operación de convolución recibe como entrada una imagen a la que aplica un filtro que devuelve un mapa de las características de la imagen original, reduciendo así el número de parámetros. La convolución se basa en las siguientes cuestiones:}
  
\CommentTok{#* Interacciones dispersas, ya que al aplicar un filtro de menor tamaño sobre la entrada original podemos reducir drásticamente la cantidad de parámetros y cálculos.}
\CommentTok{#* Parámetros compartidos entre los distintos tipos de filtros.}
\CommentTok{#* Si las entradas cambian, las salidas van a cambiar también en forma similar.}

\CommentTok{#Después de la capa convolucional, esta capa reduce en la dimensión que aunque puede conducir a una pérdida de información, también reduce los cáclulos y con ello evita el sobreajuste.}

\CommentTok{#La operación que se suele utilizar en esta capa es `max-pooling`, que divide a la imagen de entrada en un conjunto de rectángulos y, respecto de cada subregión, se va quedando con el máximo valor.}

\CommentTok{#Vamos a resolver ahora el mismo problema de identificación de dígitos. Recuerda que con una red neuronal densa obtenemos un porcentaje de acierto del 97.8%. }

\CommentTok{#Vamos ahora a construir una red neuronal convolucional básica. Utilizaremos las capas `layer_conv_2d()` y `layer_max_pooling_2d()`. }

\CommentTok{#La red neuronal convolucional recibe como entrada tensores con la siguiente forma `(altura_imagen, ancho_imagen, canal_imagen)`. Así configruramos la red neuronal convolucional para procesar entradas de tamaño `(28, 28, 1)`. Por tanto en la primera capa  `input_shape = c(28, 28, 1)` .}

\KeywordTok{library}\NormalTok{(keras)}

\NormalTok{model <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_conv_2d}\NormalTok{(}\DataTypeTok{filters =} \DecValTok{32}\NormalTok{, }\DataTypeTok{kernel_size =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DataTypeTok{activation =} \StringTok{"relu"}\NormalTok{,}
                \DataTypeTok{input_shape =} \KeywordTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_max_pooling_2d}\NormalTok{(}\DataTypeTok{pool_size =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_conv_2d}\NormalTok{(}\DataTypeTok{filters =} \DecValTok{64}\NormalTok{, }\DataTypeTok{kernel_size =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DataTypeTok{activation =} \StringTok{"relu"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_max_pooling_2d}\NormalTok{(}\DataTypeTok{pool_size =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_conv_2d}\NormalTok{(}\DataTypeTok{filters =} \DecValTok{64}\NormalTok{, }\DataTypeTok{kernel_size =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DataTypeTok{activation =} \StringTok{"relu"}\NormalTok{)}

\CommentTok{#Veamos la arquitectura de la red.}

\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## conv2d (Conv2D)                  (None, 26, 26, 32)            320         
## ___________________________________________________________________________
## max_pooling2d (MaxPooling2D)     (None, 13, 13, 32)            0           
## ___________________________________________________________________________
## conv2d_1 (Conv2D)                (None, 11, 11, 64)            18496       
## ___________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)   (None, 5, 5, 64)              0           
## ___________________________________________________________________________
## conv2d_2 (Conv2D)                (None, 3, 3, 64)              36928       
## ===========================================================================
## Total params: 55,744
## Trainable params: 55,744
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Para calcular el número de parámetros tenemos que aplicar la fórmula:}
  
\CommentTok{# total.params = (altura.filtro * ancho.filtro * numero.canales+1) * numero.filtros}

  
\CommentTok{#  El número de canales en una imagen en escala de grises es $1$, para una imagen a color, sería $3$, una por cada canal RGB.}

\CommentTok{#Así el número de parámetros de la primera capa es $(3*3*1+1)*32$ y el de la segunda $((3*3*32)+1)*64$. Fíjate que cada filtro de la primera capa se transforma en un canal de la segunda.}

\CommentTok{#La salida de `layer_conv_2d()` y `layer_max_pooling_2d()` es un 3D tensor de forma `(altura, ancho, canal)`. El ancho y la altura se reducen según se profundiza en la red. El número de canales se controla por el primer argumento que se pasa a `layer_conv_2d()` (32 o 64).}

\CommentTok{#El siguiente paso es alimentar la última capa (de tamaño `(3, 3, 64)`) en una red densa, pero como estas redes procesan 1D tensores, se deben transformar las salidas 3D en salidas 1D.}

\NormalTok{model <-}\StringTok{ }\NormalTok{model }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_flatten}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{64}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"relu"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{10}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"softmax"}\NormalTok{)}

\CommentTok{#La última capa tiene 10 neuronas porque el objetivo es realizar multicategoría, con 10 categorías.}

\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## conv2d (Conv2D)                  (None, 26, 26, 32)            320         
## ___________________________________________________________________________
## max_pooling2d (MaxPooling2D)     (None, 13, 13, 32)            0           
## ___________________________________________________________________________
## conv2d_1 (Conv2D)                (None, 11, 11, 64)            18496       
## ___________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)   (None, 5, 5, 64)              0           
## ___________________________________________________________________________
## conv2d_2 (Conv2D)                (None, 3, 3, 64)              36928       
## ___________________________________________________________________________
## flatten (Flatten)                (None, 576)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 64)                    36928       
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 10)                    650         
## ===========================================================================
## Total params: 93,322
## Trainable params: 93,322
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Es decir, cada salida con forma `(3, 3, 64)` se transforma en un vector de tamaño `(576)` antes de aplicar 2 capas densas.}

\CommentTok{#Finalmente vamos a realizar el entrenamiento.}

\NormalTok{mnist <-}\StringTok{ }\KeywordTok{dataset_mnist}\NormalTok{()}
\KeywordTok{c}\NormalTok{(}\KeywordTok{c}\NormalTok{(train_images, train_labels), }\KeywordTok{c}\NormalTok{(test_images, test_labels)) }\OperatorTok{%<-%}\StringTok{ }\NormalTok{mnist}

\NormalTok{train_images <-}\StringTok{ }\KeywordTok{array_reshape}\NormalTok{(train_images, }\KeywordTok{c}\NormalTok{(}\DecValTok{60000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train_images <-}\StringTok{ }\NormalTok{train_images }\OperatorTok{/}\StringTok{ }\DecValTok{255}

\NormalTok{test_images <-}\StringTok{ }\KeywordTok{array_reshape}\NormalTok{(test_images, }\KeywordTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{test_images <-}\StringTok{ }\NormalTok{test_images }\OperatorTok{/}\StringTok{ }\DecValTok{255}

\NormalTok{train_labels <-}\StringTok{ }\KeywordTok{to_categorical}\NormalTok{(train_labels)}
\NormalTok{test_labels <-}\StringTok{ }\KeywordTok{to_categorical}\NormalTok{(test_labels)}

\NormalTok{model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(}
  \DataTypeTok{optimizer =} \StringTok{"rmsprop"}\NormalTok{,}
  \DataTypeTok{loss =} \StringTok{"categorical_crossentropy"}\NormalTok{,}
  \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{fit}\NormalTok{(}
\NormalTok{  train_images, train_labels, }
  \DataTypeTok{epochs =} \DecValTok{5}\NormalTok{, }\DataTypeTok{batch_size=}\DecValTok{64}
\NormalTok{)}

\CommentTok{#Vamos a evaluar ahora el modelo construido.}

\NormalTok{results <-}\StringTok{ }\NormalTok{model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{evaluate}\NormalTok{(test_images, test_labels)}

\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $loss
## [1] 0.02910437
## 
## $acc
## [1] 0.9914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Como podemos observar pasamos de un porcentaje de acierto de 97.8% al 99%.}
\end{Highlighting}
\end{Shaded}


\end{document}
